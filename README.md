# LunarLanderRl â€“ Deep Reinforcement Learning Agent mit Gymnasium

Dieses Projekt wurde im Rahmen des KISY-Unterrichts erstellt. Ziel ist es, einen Deep Q-Learning Agenten zu trainieren, der das Spiel **LunarLander-v3** mithilfe der Bibliothek **gymnasium** meistert.

![LunarLander GIF](images/lunar_lander.gif)
---

## ğŸ—‚ï¸ Ordnerstruktur

```
LunarLanderRl/
â”‚
â”œâ”€â”€ main.py                 # Training des Agenten
â”œâ”€â”€ dqn_agent.py            # DQNAgent-Klasse mit Q-Network
â”œâ”€â”€ utils.py                # Plotfunktion fÃ¼r Rewards
â”œâ”€â”€ replay.py               # Spiel mit gespeichertem Modell wiedergeben
â”œâ”€â”€ features_demo.py        # Zeigt Spielzustand & Feature-Werte
â”œâ”€â”€ qvalue_demo.py          # Zeigt Q-Werte fÃ¼r fixen Zustand
â”œâ”€â”€ modell/                 # Gespeicherte Modelle (.pth)
â”œâ”€â”€ Gymnasium_Aufgabe.odt   # Word-Dokumentation
â”œâ”€â”€ images//                # Bilder fÃ¼r readme
â””â”€â”€ reward_plot.png         # Automatisch generierter Trainingsplot
```

---

## ğŸ“¦ Installation

Voraussetzungen:

- Python 3.12  
- Installation der Bibliotheken:

```bash
pip install gymnasium[box2d] torch matplotlib
```

---

## ğŸš€ Features (Inputs fÃ¼r das neuronale Netz)

Das Environment liefert 8 Zustandswerte:

1. Horizontale Position  
2. Vertikale Position  
3. Horizontale Geschwindigkeit  
4. Vertikale Geschwindigkeit  
5. Neigungswinkel  
6. Winkelgeschwindigkeit  
7. Kontakt linker FuÃŸ  
8. Kontakt rechter FuÃŸ  

Diese Features beschreiben die physikalische Lage des Landers vollstÃ¤ndig.

---

## ğŸ§  Training starten

```bash
python main.py
```

- FÃ¼hrt z.â€¯B. 10 Trainings-Episoden aus  
- Speichert das Modell in `modell/lunarlander_model.pth`  
- Zeichnet Rewards auf und erzeugt `reward_plot.png`

---

## ğŸ® Replay mit gespeichertem Modell

```bash
python replay.py
```

â†’ Gib bei der Eingabeaufforderung den Modellpfad ein, z.â€¯B.:

```
C:\...\LunarLanderRl\modell\lunarlander_model.pth
```

Der Agent spielt das Spiel basierend auf dem trainierten Q-Network.

---

## ğŸ” Beispielzustand & gewÃ¤hlte Aktion

```bash
python qvalue_demo.py
```

Zeigt:

- Einen Beispielzustand (state) mit 8 Werten  
- Die berechneten Q-Werte  
- Die vom Modell gewÃ¤hlte Aktion

---

## ğŸ“Š Feature-Demo & GUI

```bash
python features_demo.py
```

Zeigt:

- Die 8 Beobachtungswerte vom Environment  
- Das laufende Spiel im Fenster (env.render)  

---

## ğŸ§® Wie funktionieren die Rewards bei LunarLander?

Das Spiel **belohnt** oder **bestraft** Aktionen basierend auf physikalisch sinnvollem Verhalten. Der Agent soll weich auf einer Plattform landen â€“ alles andere wird bestraft.

### âœ… Positive Rewards (Belohnungen)

| Aktion                                   | Belohnung           | Beschreibung |
|------------------------------------------|----------------------|--------------|
| Sanfte Landung auf der Plattform         | +100 bis +140        | Wenn beide Beine sicher Bodenkontakt haben. |
| Landebeine berÃ¼hren Boden                | +10 pro Bein         | Auch bei Zwischenlandungen gibt es Punkte. |
| Guter Flug in Richtung Plattform         | +0 bis +100          | Belohnung fÃ¼r zentriertes, langsames Sinken. |

---

### âŒ Negative Rewards (Strafen)

| Aktion                                   | Strafe              | Beschreibung |
|------------------------------------------|----------------------|--------------|
| Absturz (explodieren)                    | -100 bis -150        | Bei harter oder schiefer Landung. |
| Fliegen ohne zu landen                   | -0.03 pro Frame      | Dauerhafte Strafe, um Effizienz zu erzwingen. |
| Aus dem Bild fliegen                     | -100                 | Wenn das Schiff das Spielfeld verlÃ¤sst. |
| Hartes Aufprallen mit einem Bein         | -5 bis -10           | Hohe AufprallkrÃ¤fte fÃ¼hren zu Strafe. |

---

### ğŸ“‰ Typische Reward-Werte

| Trainingsstand     | Gesamt-Reward pro Episode | Bedeutung |
|--------------------|---------------------------|-----------|
| Sehr schlecht      | < -2000                   | ZufÃ¤llige Aktionen, schneller Absturz |
| Lernanfang         | -1000 bis -300            | Agent bleibt lÃ¤nger in der Luft |
| Erste Landungen    | -100 bis +100             | Agent lernt kontrollierte ManÃ¶ver |
| Gut trainiert      | +200 oder mehr            | Agent landet regelmÃ¤ÃŸig sicher |

---

ğŸ“Œ Hinweis:  
Die Rewards wirken klein, aber sie summieren sich Ã¼ber **Hunderte Zeitschritte pro Episode**. Deshalb ist das **Gesamtverhalten** entscheidend â€“ nicht einzelne Frames.

```python
# Beispiel zur Reward-Anzeige wÃ¤hrend Training
print(f"Episode {ep+1}: Reward = {total_reward:.2f}")
```
Beispiel nach 500 Episoden
![Beispiel reward](images/reward.png)

## ğŸ“¸ Screenshots

Die folgenden Screenshots wurden im Rahmen der Theorieaufgaben erstellt:

### Zustand + Q-Werte (fÃ¼r "Deep Q-Learning definieren")
![Beispiel Q-Werte](images/qvalues.png)

Der Agent bekommt einen Beispielzustand und berechnet die dazugehÃ¶rigen Q-Werte. Daraus wÃ¤hlt er die beste Aktion â€“ in diesem Fall z.â€¯B. das ZÃ¼nden der HauptdÃ¼se (Aktion 2).

### GUI wÃ¤hrend des Spiels
![LunarLander GUI](images/gui_demo.png)

Das Spiel zeigt die Position des Landers wÃ¤hrend des Trainings oder Replays. Die Beobachtungswerte sind im Terminal sichtbar (`features_demo.py`).

---

## ğŸ¥ Video

Das folgende Video zeigt den Trainingsverlauf und ein Beispiel-Replay eines trainierten Agenten.

### YouTube

[![Replay anschauen](https://img.youtube.com/vi/YOUTUBE_VIDEO_ID/0.jpg)](https://www.youtube.com/watch?v=YOUTUBE_VIDEO_ID)




